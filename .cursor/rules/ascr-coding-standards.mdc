---
description: 
globs: 
alwaysApply: false
---
# ASCR Project Coding Standards & Rules

## ğŸ¯ Project Overview
ASCR (Automated Standard Construction Reference) is a Korean PDF document automation processing system. The project follows specific architectural patterns and coding conventions optimized for Korean language processing and PDF manipulation.

## ğŸ“ Project Structure
- **Entry Point**: [main.py](mdc:main.py) - Main execution file with interactive menu
- **Configuration**: [config/settings.py](mdc:config/settings.py) - Central configuration management
- **Core Logic**: [src/](mdc:src) - Modular source code organization
- **Dependencies**: [requirements.txt](mdc:requirements.txt) - Fixed version dependencies

## ğŸš¨ CRITICAL RULES - MUST FOLLOW

### 1. File Header & Encoding (MANDATORY)
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Module description in Korean
"""
```
**ALWAYS** include these three elements in every Python file.

### 2. Cross-Platform Compatibility (MANDATORY)
```python
import platform
import sys
from pathlib import Path

# ALWAYS use pathlib.Path for cross-platform path handling
def get_platform_info() -> Dict[str, str]:
    """í”Œë«í¼ ì •ë³´ ë°˜í™˜"""
    return {
        "os": platform.system(),
        "version": platform.version(),
        "python_version": sys.version,
        "architecture": platform.architecture()[0]
    }

# ALWAYS use Path.joinpath() or / operator for path concatenation
def create_cross_platform_path(base_dir: str, filename: str) -> Path:
    """í¬ë¡œìŠ¤ í”Œë«í¼ ê²½ë¡œ ìƒì„±"""
    return Path(base_dir) / filename

# ALWAYS handle line endings properly
def read_file_cross_platform(file_path: Path) -> str:
    """í¬ë¡œìŠ¤ í”Œë«í¼ íŒŒì¼ ì½ê¸°"""
    with open(file_path, 'r', encoding='utf-8', newline='') as f:
        return f.read()

# ALWAYS use os.pathsep for PATH environment variables
import os
def get_executable_path() -> str:
    """ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    if platform.system() == "Windows":
        return "python.exe"
    else:
        return "python3"
```

**NEVER** use hardcoded path separators (`\` or `/`).

### 3. Korean Language Support (MANDATORY)
- Use Korean docstrings and comments for all user-facing documentation
- Use `encoding='utf-8-sig'` for CSV files to ensure Korean character compatibility
- All error messages and user feedback must be in Korean

### 4. Type Hints (MANDATORY)
```python
from typing import List, Dict, Optional, Tuple

def process_pdf(self, input_path: str) -> None:
def extract_levels(code: str) -> Dict[str, str]:
```
**NEVER** create functions without type hints.

### 5. Error Handling Pattern (MANDATORY)
```python
try:
    # Main logic
except FileNotFoundError:
    print(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    return []
except Exception as e:
    print(f"ì˜¤ë¥˜: ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return []
```
**ALWAYS** handle specific exceptions first, then general exceptions.

### 6. Path Management (MANDATORY)
```python
from pathlib import Path
# Use Path objects, NEVER os.path for file operations
```
**NEVER** use `os.path` - always use `pathlib.Path`.

### 7. Document Analysis Pattern (MANDATORY)
```python
class DocumentAnalyzer:
    """ë¬¸ì„œ ë¶„ì„ í´ë˜ìŠ¤ - ë°˜ë“œì‹œ ì‚¬ìš©"""
    
    def __init__(self, file_path: Path):
        self.file_path = file_path
        self.content = ""
        self.structure = {}
        
    def analyze_document_structure(self) -> Dict[str, Any]:
        """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ - ì²´ê³„ì  ì ‘ê·¼ í•„ìˆ˜"""
        try:
            # 1. íŒŒì¼ íƒ€ì… ë° í¬ê¸° í™•ì¸
            file_info = self._get_file_info()
            
            # 2. ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 1000ì)
            preview = self._get_content_preview()
            
            # 3. êµ¬ì¡°ì  íŒ¨í„´ ê²€ìƒ‰
            patterns = self._find_structural_patterns()
            
            # 4. ë¶€ë¬¸/ì¥/ì ˆ êµ¬ë¶„ ë¶„ì„
            sections = self._analyze_sections()
            
            # 5. ê²€ì¦ ë° ê²°ê³¼ ë°˜í™˜
            return self._validate_and_return(file_info, preview, patterns, sections)
            
        except Exception as e:
            print(f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {}
    
    def _get_file_info(self) -> Dict[str, Any]:
        """íŒŒì¼ ì •ë³´ ìˆ˜ì§‘"""
        return {
            "name": self.file_path.name,
            "size": self.file_path.stat().st_size,
            "type": self.file_path.suffix.lower(),
            "exists": self.file_path.exists()
        }
    
    def _get_content_preview(self) -> str:
        """ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°"""
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                return f.read(1000)
        except UnicodeDecodeError:
            # ë°”ì´ë„ˆë¦¬ íŒŒì¼ì¸ ê²½ìš°
            return "[ë°”ì´ë„ˆë¦¬ íŒŒì¼]"
    
    def _find_structural_patterns(self) -> Dict[str, List[str]]:
        """êµ¬ì¡°ì  íŒ¨í„´ ê²€ìƒ‰"""
        patterns = {
            "ë¶€ë¬¸": [],
            "ì¥": [],
            "ì ˆ": [],
            "í˜ì´ì§€": []
        }
        
        # ì •ê·œì‹ íŒ¨í„´ ì •ì˜
        section_patterns = {
            "ë¶€ë¬¸": r"([ê°€-í£]+ë¶€ë¬¸)",
            "ì¥": r"(ì œ[0-9]+ì¥\s*[ê°€-í£]+)",
            "ì ˆ": r"([0-9]+-[0-9]+[ê°€-í£]*)",
            "í˜ì´ì§€": r"(=== [0-9]+í˜ì´ì§€ ==)"
        }
        
        # íŒ¨í„´ ê²€ìƒ‰
        for pattern_name, pattern in section_patterns.items():
            matches = re.findall(pattern, self.content)
            patterns[pattern_name] = matches
        
        return patterns
    
    def _analyze_sections(self) -> Dict[str, Any]:
        """ë¶€ë¬¸/ì¥/ì ˆ êµ¬ë¶„ ë¶„ì„"""
        sections = {
            "ê³µí†µë¶€ë¬¸": {"start": None, "end": None, "chapters": []},
            "í† ëª©ë¶€ë¬¸": {"start": None, "end": None, "chapters": []},
            "ê±´ì¶•ë¶€ë¬¸": {"start": None, "end": None, "chapters": []},
            "ê¸°ê³„ì„¤ë¹„ë¶€ë¬¸": {"start": None, "end": None, "chapters": []},
            "ìœ ì§€ê´€ë¦¬ë¶€ë¬¸": {"start": None, "end": None, "chapters": []}
        }
        
        # ë¶€ë¬¸ ì‹œì‘ì  ì°¾ê¸°
        for line_num, line in enumerate(self.content.split('\n')):
            for section_name in sections.keys():
                if section_name in line:
                    sections[section_name]["start"] = line_num
                    break
        
        return sections
    
    def _validate_and_return(self, file_info: Dict, preview: str, 
                           patterns: Dict, sections: Dict) -> Dict[str, Any]:
        """ê²€ì¦ ë° ê²°ê³¼ ë°˜í™˜"""
        return {
            "file_info": file_info,
            "preview": preview[:200] + "..." if len(preview) > 200 else preview,
            "patterns": patterns,
            "sections": sections,
            "analysis_confidence": self._calculate_confidence(patterns, sections)
        }
    
    def _calculate_confidence(self, patterns: Dict, sections: Dict) -> float:
        """ë¶„ì„ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.0
        
        # ë¶€ë¬¸ì´ ë°œê²¬ë˜ë©´ +30%
        if any(sections[section]["start"] is not None for section in sections):
            confidence += 0.3
        
        # ì¥ì´ ë°œê²¬ë˜ë©´ +30%
        if patterns["ì¥"]:
            confidence += 0.3
        
        # ì ˆì´ ë°œê²¬ë˜ë©´ +20%
        if patterns["ì ˆ"]:
            confidence += 0.2
        
        # í˜ì´ì§€ êµ¬ë¶„ì´ ë°œê²¬ë˜ë©´ +20%
        if patterns["í˜ì´ì§€"]:
            confidence += 0.2
        
        return min(confidence, 1.0)
```

### 8. PDF Processing Rules (MANDATORY)
```python
# ALWAYS distinguish between different PDF types
class PDFProcessor:
    """PDF ì²˜ë¦¬ í´ë˜ìŠ¤ - ë°˜ë“œì‹œ ì‚¬ìš©"""
    
    def __init__(self):
        self.toc_pdf = "input/split_3_49.pdf"  # ëª©ì°¨ë§Œ í¬í•¨ (47í˜ì´ì§€)
        self.full_pdf = "input/2025_construction_work_standard_price_list.pdf"  # ì „ì²´ ë‚´ìš© í¬í•¨
    
    def process_toc_extraction(self) -> Dict[str, Any]:
        """ëª©ì°¨ ì¶”ì¶œìš© PDF ì²˜ë¦¬"""
        # split_3_49.pdfëŠ” ëª©ì°¨ êµ¬ì¡° ë¶„ì„ìš©
        return self._extract_toc_structure(self.toc_pdf)
    
    def process_full_content_split(self) -> Dict[str, Any]:
        """ì „ì²´ ë‚´ìš© ë¶„í• ìš© PDF ì²˜ë¦¬"""
        # 2025_construction_work_standard_price_list.pdfëŠ” ì‹¤ì œ ë¶„í• ìš©
        return self._split_full_content(self.full_pdf)
    
    def _validate_pdf_purpose(self, pdf_path: str) -> str:
        """PDF ìš©ë„ ê²€ì¦"""
        if "split_3_49" in pdf_path:
            return "toc_extraction"  # ëª©ì°¨ ì¶”ì¶œìš©
        elif "2025_construction_work_standard_price_list" in pdf_path:
            return "full_content_split"  # ì „ì²´ ë‚´ìš© ë¶„í• ìš©
        else:
            return "unknown"
```

**NEVER** use the wrong PDF file for the wrong purpose.

### 9. PDF Split Workflow (MANDATORY)
```python
class PDFSplitWorkflow:
    """PDF ë¶„í•  ì›Œí¬í”Œë¡œìš° - ë°˜ë“œì‹œ ì‚¬ìš©"""
    
    def __init__(self):
        self.toc_json = None
        self.full_pdf_path = "input/2025_construction_work_standard_price_list.pdf"
    
    def execute_split_workflow(self) -> bool:
        """PDF ë¶„í•  ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        try:
            # 1ë‹¨ê³„: ëª©ì°¨ JSON ë¡œë“œ (split_3_49.pdfì—ì„œ ìƒì„±ëœ ê²ƒ)
            if not self._load_toc_json():
                return False
            
            # 2ë‹¨ê³„: ì „ì²´ PDF ë¡œë“œ (2025_construction_work_standard_price_list.pdf)
            if not self._load_full_pdf():
                return False
            
            # 3ë‹¨ê³„: ë¶€ë¬¸ë³„/ì¥ë³„ ë¶„í• 
            return self._split_by_sections()
            
        except Exception as e:
            print(f"PDF ë¶„í•  ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}")
            return False
    
    def _load_toc_json(self) -> bool:
        """ëª©ì°¨ JSON ë¡œë“œ"""
        json_path = "output/toc_tree_20250621_001431.json"
        if not Path(json_path).exists():
            print(f"ì˜¤ë¥˜: ëª©ì°¨ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {json_path}")
            return False
        
        with open(json_path, 'r', encoding='utf-8') as f:
            self.toc_json = json.load(f)
        return True
    
    def _load_full_pdf(self) -> bool:
        """ì „ì²´ PDF ë¡œë“œ"""
        if not Path(self.full_pdf_path).exists():
            print(f"ì˜¤ë¥˜: ì „ì²´ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.full_pdf_path}")
            return False
        
        self.pdf_reader = PdfReader(self.full_pdf_path)
        print(f"âœ… ì „ì²´ PDF ë¡œë“œ ì™„ë£Œ: {len(self.pdf_reader.pages)}í˜ì´ì§€")
        return True
```

**ALWAYS** use the correct PDF file for each purpose.

### 10. Table of Contents Tree Structure Generation (MANDATORY)
```python
class TableOfContentsGenerator:
    """ëª©ì°¨ íŠ¸ë¦¬ êµ¬ì¡° ìƒì„± í´ë˜ìŠ¤ - ë°˜ë“œì‹œ ì‚¬ìš©"""
    
    def parse_toc_tree(self, content: str) -> Dict[str, Any]:
        """ëª©ì°¨ íŠ¸ë¦¬ êµ¬ì¡° ìƒì„± - ì¥ ë²ˆí˜¸ì™€ ì œëª© ì—°ê²° í•„ìˆ˜"""
        
        # 1. ì¥ ë²ˆí˜¸ì™€ ì¥ ì œëª©ì´ ì—°ì†ëœ ë‘ ì¤„ì— ë¶„ë¦¬ë˜ì–´ ìˆì„ ê²½ìš°, 
        #    ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ë…¸ë“œ(ì œ1ì¥ ì ìš©ê¸°ì¤€)ë¡œ í•©ì³ì„œ íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ìƒì„±í•´ì•¼ í•œë‹¤.
        chapter_pattern = re.compile(r'^(\d+)ì¤„: ì œ(\d+)ì¥$')
        title_pattern = re.compile(r'^(\d+)ì¤„: ([ê°€-í£A-Za-z0-9\-\s]+)\s*Â·+\s*(\d+)$')
        
        # 2. ì‹¤ì œ ë°ì´í„° ìƒ˜í”Œì„ ê¸°ë°˜ìœ¼ë¡œ, ì¥/ì ˆ/ì¡° ë“± ëª¨ë“  ê³„ì¸µì´ ì˜¬ë°”ë¥´ê²Œ 
        #    íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ë³€í™˜ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ì‘ì„±í•˜ê³ , ê²°ê³¼ë¥¼ ê²€ì¦í•œë‹¤.
        chapters = {}
        current_chapter = None
        
        for line in content.split('\n'):
            # ì¥ ë²ˆí˜¸ ë§¤ì¹­
            chapter_match = chapter_pattern.match(line.strip())
            if chapter_match:
                line_num = int(chapter_match.group(1))
                chapter_num = chapter_match.group(2)
                current_chapter = f"ì œ{chapter_num}ì¥"
                chapters[current_chapter] = {"title": "", "page": None, "line_num": line_num}
                continue
            
            # ì¥ ì œëª© ë§¤ì¹­ (ë‹¤ìŒ ì¤„)
            if current_chapter and chapters[current_chapter]["title"] == "":
                title_match = title_pattern.match(line.strip())
                if title_match:
                    title = title_match.group(2).strip()
                    page = int(title_match.group(3))
                    chapters[current_chapter]["title"] = title
                    chapters[current_chapter]["page"] = page
                    # ì¥ ë²ˆí˜¸ì™€ ì œëª©ì„ í•˜ë‚˜ì˜ ë…¸ë“œë¡œ í•©ì¹¨
                    chapters[current_chapter]["full_title"] = f"{current_chapter} {title}"
        
        # 3. ë§ˆí¬ë‹¤ìš´ ë“± ì¶œë ¥ í¬ë§·ì—ì„œëŠ” ì¥ ë²ˆí˜¸ì™€ ì œëª©ì´ ëª¨ë‘ í¬í•¨ëœ í˜•íƒœ
        #    (ì œ1ì¥ ì ìš©ê¸°ì¤€)ë¡œ ì¶œë ¥ë˜ë„ë¡ í•œë‹¤.
        return chapters
```

**NEVER** create separate nodes for chapter numbers and titles - always combine them.

### 11. Document Analysis Validation (MANDATORY - NEW)
```python
class DocumentAnalysisValidator:
    """ë¬¸ì„œ ë¶„ì„ ê²€ì¦ í´ë˜ìŠ¤ - ë°˜ë“œì‹œ ì‚¬ìš©"""
    
    def __init__(self):
        self.required_sections = [
            "ê³µí†µë¶€ë¬¸", "í† ëª©ë¶€ë¬¸", "ê±´ì¶•ë¶€ë¬¸", 
            "ê¸°ê³„ì„¤ë¹„ë¶€ë¬¸", "ìœ ì§€ê´€ë¦¬ë¶€ë¬¸"
        ]
        self.validation_errors = []
        
    def validate_analysis_completeness(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ì™„ì „ì„± ê²€ì¦ - ë°˜ë“œì‹œ ì‹¤í–‰"""
        validation_result = {
            "is_complete": True,
            "missing_sections": [],
            "validation_errors": [],
            "recommendations": []
        }
        
        # 1. í•„ìˆ˜ ë¶€ë¬¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        sections = analysis_results.get("sections", {})
        for required_section in self.required_sections:
            if not sections.get(required_section, {}).get("start"):
                validation_result["missing_sections"].append(required_section)
                validation_result["is_complete"] = False
        
        # 2. ë¶€ë¬¸ë³„ ì¥(ç« ) ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        for section_name, section_data in sections.items():
            if section_data.get("start") is not None:
                chapters = section_data.get("chapters", [])
                if not chapters:
                    validation_result["validation_errors"].append(
                        f"{section_name}: ì¥(ç« ) ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"
                    )
        
        # 3. í˜ì´ì§€ ë²”ìœ„ ê²€ì¦
        self._validate_page_ranges(analysis_results, validation_result)
        
        # 4. ê¶Œì¥ì‚¬í•­ ìƒì„±
        validation_result["recommendations"] = self._generate_recommendations(
            validation_result
        )
        
        return validation_result
    
    def _validate_page_ranges(self, analysis_results: Dict[str, Any], 
                            validation_result: Dict[str, Any]) -> None:
        """í˜ì´ì§€ ë²”ìœ„ ê²€ì¦"""
        sections = analysis_results.get("sections", {})
        content_lines = analysis_results.get("content", "").split('\n')
        
        for section_name, section_data in sections.items():
            if section_data.get("start") is not None:
                start_line = section_data["start"]
                end_line = section_data.get("end")
                
                # ì‹œì‘ì  ì´í›„ 100ì¤„ ë‚´ì— ë‹¤ìŒ ë¶€ë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸
                next_section_found = False
                for i in range(start_line + 1, min(start_line + 100, len(content_lines))):
                    for other_section in self.required_sections:
                        if other_section != section_name and other_section in content_lines[i]:
                            next_section_found = True
                            break
                    if next_section_found:
                        break
                
                if not next_section_found and end_line is None:
                    validation_result["validation_errors"].append(
                        f"{section_name}: ëì ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                    )
    
    def _generate_recommendations(self, validation_result: Dict[str, Any]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if validation_result["missing_sections"]:
            recommendations.append(
                f"ëˆ„ë½ëœ ë¶€ë¬¸ì„ í™•ì¸í•˜ì„¸ìš”: {', '.join(validation_result['missing_sections'])}"
            )
        
        if validation_result["validation_errors"]:
            recommendations.append(
                "ê²€ì¦ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”: " + "; ".join(validation_result["validation_errors"])
            )
        
        if not validation_result["is_complete"]:
            recommendations.append("ì „ì²´ ë¬¸ì„œë¥¼ ë‹¤ì‹œ ë¶„ì„í•˜ì„¸ìš”")
        
        return recommendations
```

### 12. Analysis Report Generator (MANDATORY - NEW)
```python
class AnalysisReportGenerator:
    """ë¶„ì„ ë³´ê³ ì„œ ìë™ ìƒì„± í´ë˜ìŠ¤ - ë°˜ë“œì‹œ ì‚¬ìš©"""
    
    def __init__(self, analysis_results: Dict[str, Any], validation_results: Dict[str, Any]):
        self.analysis_results = analysis_results
        self.validation_results = validation_results
        
    def generate_comprehensive_report(self) -> str:
        """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„± - ë°˜ë“œì‹œ ì‹¤í–‰"""
        report_parts = []
        
        # 1. ë¶„ì„ ê°œìš”
        report_parts.append(self._generate_overview())
        
        # 2. ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
        report_parts.append(self._generate_structure_analysis())
        
        # 3. ë¶€ë¬¸ë³„ ìƒì„¸ ë¶„ì„
        report_parts.append(self._generate_section_details())
        
        # 4. ê²€ì¦ ê²°ê³¼
        report_parts.append(self._generate_validation_report())
        
        # 5. ê¶Œì¥ì‚¬í•­
        report_parts.append(self._generate_recommendations())
        
        return "\n\n".join(report_parts)
    
    def _generate_overview(self) -> str:
        """ë¶„ì„ ê°œìš” ìƒì„±"""
        file_info = self.analysis_results.get("file_info", {})
        return f"""## ğŸ“‹ PDF êµ¬ì¡° ë¶„ì„ ë³´ê³ ì„œ

### ğŸ¯ **ë¶„ì„ ê°œìš”**
- **íŒŒì¼ëª…**: {file_info.get('name', 'N/A')}
- **ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **íŒŒì¼ í¬ê¸°**: {file_info.get('size', 0):,} bytes
- **ë¶„ì„ ì‹ ë¢°ë„**: {self.analysis_results.get('analysis_confidence', 0):.1f}%"""
    
    def _generate_structure_analysis(self) -> str:
        """êµ¬ì¡° ë¶„ì„ ìƒì„±"""
        sections = self.analysis_results.get("sections", {})
        structure_text = "### ğŸ—ï¸ **ë¬¸ì„œ êµ¬ì¡° ë¶„ì„**\n\n"
        
        for section_name, section_data in sections.items():
            if section_data.get("start") is not None:
                chapters = section_data.get("chapters", [])
                structure_text += f"**{section_name}**\n"
                for chapter in chapters:
                    structure_text += f"- {chapter}\n"
                structure_text += "\n"
        
        return structure_text
    
    def _generate_section_details(self) -> str:
        """ë¶€ë¬¸ë³„ ìƒì„¸ ë¶„ì„ ìƒì„±"""
        sections = self.analysis_results.get("sections", {})
        details_text = "### ğŸ“Š **ë¶€ë¬¸ë³„ ìƒì„¸ ë¶„ì„**\n\n"
        
        for section_name, section_data in sections.items():
            if section_data.get("start") is not None:
                start_line = section_data["start"]
                end_line = section_data.get("end", "ë¯¸ì •")
                chapters = section_data.get("chapters", [])
                
                details_text += f"**{section_name}**\n"
                details_text += f"- ì‹œì‘ ìœ„ì¹˜: {start_line}ë²ˆì§¸ ì¤„\n"
                details_text += f"- ë ìœ„ì¹˜: {end_line}ë²ˆì§¸ ì¤„\n"
                details_text += f"- í¬í•¨ëœ ì¥: {len(chapters)}ê°œ\n"
                details_text += "\n"
        
        return details_text
    
    def _generate_validation_report(self) -> str:
        """ê²€ì¦ ê²°ê³¼ ìƒì„±"""
        validation = self.validation_results
        validation_text = "### âœ… **ê²€ì¦ ê²°ê³¼**\n\n"
        
        if validation.get("is_complete", False):
            validation_text += "âœ… **ë¶„ì„ ì™„ë£Œ**: ëª¨ë“  í•„ìˆ˜ ë¶€ë¬¸ì´ ì •ìƒì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
        else:
            validation_text += "âŒ **ë¶„ì„ ë¶ˆì™„ì „**: ì¼ë¶€ ë¶€ë¬¸ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
        
        if validation.get("missing_sections"):
            validation_text += f"**ëˆ„ë½ëœ ë¶€ë¬¸**: {', '.join(validation['missing_sections'])}\n\n"
        
        if validation.get("validation_errors"):
            validation_text += "**ê²€ì¦ ì˜¤ë¥˜**:\n"
            for error in validation["validation_errors"]:
                validation_text += f"- {error}\n"
            validation_text += "\n"
        
        return validation_text
    
    def _generate_recommendations(self) -> str:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = self.validation_results.get("recommendations", [])
        rec_text = "### ğŸ’¡ **ê¶Œì¥ì‚¬í•­**\n\n"
        
        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                rec_text += f"{i}. {rec}\n"
        else:
            rec_text += "í˜„ì¬ ë¶„ì„ ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.\n"
        
        return rec_text
```

### 13. Mandatory Analysis Workflow (MANDATORY - NEW)
```python
def execute_mandatory_analysis_workflow(file_path: Path) -> Dict[str, Any]:
    """í•„ìˆ˜ ë¶„ì„ ì›Œí¬í”Œë¡œìš° - ë°˜ë“œì‹œ ì‚¬ìš©"""
    try:
        # 1. ë¬¸ì„œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = DocumentAnalyzer(file_path)
        
        # 2. ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
        analysis_results = analyzer.analyze_document_structure()
        
        # 3. ê²€ì¦ê¸° ì´ˆê¸°í™” ë° ê²€ì¦
        validator = DocumentAnalysisValidator()
        validation_results = validator.validate_analysis_completeness(analysis_results)
        
        # 4. ë³´ê³ ì„œ ìƒì„±ê¸° ì´ˆê¸°í™” ë° ë³´ê³ ì„œ ìƒì„±
        report_generator = AnalysisReportGenerator(analysis_results, validation_results)
        comprehensive_report = report_generator.generate_comprehensive_report()
        
        # 5. ê²°ê³¼ ë°˜í™˜
        return {
            "analysis_results": analysis_results,
            "validation_results": validation_results,
            "comprehensive_report": comprehensive_report,
            "is_valid": validation_results.get("is_complete", False)
        }
        
    except Exception as e:
        print(f"ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {
            "error": str(e),
            "is_valid": False
        }
```

**ğŸš¨ CRITICAL: ëª¨ë“  ë¬¸ì„œ ë¶„ì„ ì‘ì—…ì€ ë°˜ë“œì‹œ ìœ„ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.**

## ğŸš¨ CRITICAL: ê±´ì¶•ë¶€ë¬¸ ëˆ„ë½ ì‚¬ê³ ì™€ ê°™ì€ ë¶„ì„ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ìœ„ì˜ ëª¨ë“  ê²€ì¦ ê·œì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨ íŠ¸ë¦¬ ìƒì„± ê·œì¹™ (MANDATORY - NEW)

### 1. ëª©ì°¨ íŠ¸ë¦¬ ì™„ì „ì„± ê²€ì¦ (MANDATORY)
```python
def validate_toc_completeness(pdf_pages: int, toc_tree: dict) -> Dict[str, Any]:
    """ëª©ì°¨ íŠ¸ë¦¬ ì™„ì „ì„± ê²€ì¦ - ë°˜ë“œì‹œ ì‹¤í–‰"""
    validation_result = {
        "is_complete": True,
        "missing_pages": [],
        "found_pages": [],
        "total_pages_in_tree": 0,
        "validation_errors": []
    }
    
    # PDF ì´ í˜ì´ì§€ ìˆ˜ì™€ íŠ¸ë¦¬ ë‚´ í˜ì´ì§€ ìˆ˜ ë¹„êµ
    tree_pages = set()
    def collect_pages(node):
        if 'page' in node:
            tree_pages.add(node['page'])
        for child in node.get('children', []):
            collect_pages(child)
    
    collect_pages(toc_tree)
    validation_result["found_pages"] = sorted(list(tree_pages))
    validation_result["total_pages_in_tree"] = len(tree_pages)
    
    # ëˆ„ë½ëœ í˜ì´ì§€ í™•ì¸
    for page_num in range(1, pdf_pages + 1):
        if page_num not in tree_pages:
            validation_result["missing_pages"].append(page_num)
            validation_result["is_complete"] = False
    
    return validation_result
```

### 2. ëª©ì°¨ íŒ¨í„´ ì¸ì‹ ê·œì¹™ (MANDATORY)
```python
# ğŸš¨ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•  ëª©ì°¨ íŒ¨í„´ ì¸ì‹ ê·œì¹™

# 1. ì¥ íŒ¨í„´: "ì œnì¥ ì œëª©" í˜•íƒœ
chapter_pattern = re.compile(r'^(\d+)ì¤„: ì œ(\d+)ì¥\s+(.+?)\s+Â·Â·Â·\s+(\d+)$')

# 2. ì ˆ/ì¡°/í•­ëª© íŒ¨í„´: "n-n-n ì œëª©" í˜•íƒœ  
item_pattern = re.compile(r'^(\d+)ì¤„: (\d+-\d+(-\d+)?)\s+(.+?)\s+Â·Â·Â·\s+(\d+)$')

# 3. ê¸°íƒ€ í•­ëª© íŒ¨í„´: "ì°¸ê³ ìë£Œ", "ë¶€ë¡", "ì‚­ì œì˜ˆì •í•­ëª©" ë“±
other_pattern = re.compile(r'^(\d+)ì¤„: (.+?)\s+Â·Â·Â·\s+(\d+)$')

# 4. ëª¨ë“  í˜ì´ì§€ì˜ ëª¨ë“  í•­ëª©ì„ ë°˜ë“œì‹œ ì¸ì‹í•´ì•¼ í•¨
# 5. ìˆ«ì íŒ¨í„´ì´ ì•„ë‹Œ ê¸°íƒ€ í•­ëª©ë„ íŠ¸ë¦¬ì— í¬í•¨í•´ì•¼ í•¨
# 6. ëˆ„ë½ëœ í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¬ë¶„ì„ ìˆ˜í–‰
```

### 3. ëª©ì°¨ íŠ¸ë¦¬ ìƒì„± ì›Œí¬í”Œë¡œìš° (MANDATORY)
```python
def execute_mandatory_toc_workflow(pdf_path: str) -> Dict[str, Any]:
    """í•„ìˆ˜ ëª©ì°¨ íŠ¸ë¦¬ ìƒì„± ì›Œí¬í”Œë¡œìš° - ë°˜ë“œì‹œ ì‚¬ìš©"""
    try:
        # 1ë‹¨ê³„: PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extractor = PDFTextExtractor(pdf_path)
        content = extractor.extract_text_by_page()
        
        # 2ë‹¨ê³„: ëª©ì°¨ í˜ì´ì§€ ì‹ë³„
        toc_pages = extractor.identify_toc_pages()
        
        # 3ë‹¨ê³„: ëª©ì°¨ íŠ¸ë¦¬ ìƒì„±
        parser = TOCParser()
        toc_tree = parser.parse_toc_tree(content, toc_pages)
        
        # 4ë‹¨ê³„: ì™„ì „ì„± ê²€ì¦
        pdf_page_count = extractor.get_total_pages()
        completeness_validation = validate_toc_completeness(pdf_page_count, toc_tree)
        
        # 5ë‹¨ê³„: ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¬ë¶„ì„
        if not completeness_validation["is_complete"]:
            print(f"âš ï¸ ê²½ê³ : {len(completeness_validation['missing_pages'])}ê°œ í˜ì´ì§€ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"ëˆ„ë½ëœ í˜ì´ì§€: {completeness_validation['missing_pages']}")
            # ì¬ë¶„ì„ ë¡œì§ ì‹¤í–‰
            toc_tree = parser.parse_toc_tree_enhanced(content, toc_pages)
            completeness_validation = validate_toc_completeness(pdf_page_count, toc_tree)
        
        # 6ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ ìƒì„±
        markdown_generator = TOCMarkdownGenerator()
        markdown_content = markdown_generator.generate_markdown(toc_tree)
        
        # 7ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ë°˜í™˜
        final_result = {
            "toc_tree": toc_tree,
            "completeness_validation": completeness_validation,
            "markdown_content": markdown_content,
            "total_pages": pdf_page_count,
            "pages_in_tree": completeness_validation["total_pages_in_tree"],
            "is_complete": completeness_validation["is_complete"]
        }
        
        return final_result
        
    except Exception as e:
        print(f"âŒ ëª©ì°¨ íŠ¸ë¦¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {
            "error": str(e),
            "is_complete": False
        }
```

### 4. ì˜¤ë¥˜ ë°©ì§€ ê·œì¹™ (MANDATORY)
```python
# ğŸš¨ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•  ëª©ì°¨ íŠ¸ë¦¬ ì˜¤ë¥˜ ë°©ì§€ ê·œì¹™

# 1. ëª¨ë“  ëª©ì°¨ íŠ¸ë¦¬ ìƒì„±ì€ execute_mandatory_toc_workflow() ì‚¬ìš©
# 2. ìƒì„±ëœ íŠ¸ë¦¬ëŠ” ë°˜ë“œì‹œ validate_toc_completeness() ê²€ì¦
# 3. PDF í˜ì´ì§€ ìˆ˜ì™€ íŠ¸ë¦¬ ë‚´ í˜ì´ì§€ ìˆ˜ê°€ ì¼ì¹˜í•´ì•¼ í•¨
# 4. ëˆ„ë½ëœ í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¬ë¶„ì„ ìˆ˜í–‰
# 5. ê¸°íƒ€ í•­ëª©(ì°¸ê³ ìë£Œ, ë¶€ë¡ ë“±)ë„ ë°˜ë“œì‹œ íŠ¸ë¦¬ì— í¬í•¨
# 6. ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ìì—ê²Œ ëª…í™•í•œ ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥

def prevent_toc_errors(pdf_path: str) -> Dict[str, Any]:
    """ëª©ì°¨ íŠ¸ë¦¬ ì˜¤ë¥˜ ë°©ì§€ í•¨ìˆ˜ - ë°˜ë“œì‹œ ì‚¬ìš©"""
    # í•„ìˆ˜ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    result = execute_mandatory_toc_workflow(pdf_path)
    
    # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ìë™ ì¬ë¶„ì„
    if not result.get("is_complete", False):
        print("ğŸ”„ ëª©ì°¨ íŠ¸ë¦¬ ê²€ì¦ ì‹¤íŒ¨ë¡œ ì¸í•œ ì¬ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        # ì¬ë¶„ì„ ë¡œì§
        result = execute_mandatory_toc_workflow(pdf_path)
    
    return result
```

**ğŸš¨ CRITICAL: 47í˜ì´ì§€ ëˆ„ë½ ì‚¬ê³ ì™€ ê°™ì€ ëª©ì°¨ íŠ¸ë¦¬ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ìœ„ì˜ ëª¨ë“  ê²€ì¦ ê·œì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.**

## ğŸ“Š ëª©ì°¨ íŠ¸ë¦¬ ê³„ì¸µ êµ¬ì¡° ê·œì¹™ (MANDATORY - NEW)

### 1. ê³„ì¸µ êµ¬ì¡° ì •ì˜ (MANDATORY)
```python
class TOCHierarchyRules:
    """ëª©ì°¨ íŠ¸ë¦¬ ê³„ì¸µ êµ¬ì¡° ê·œì¹™ - ë°˜ë“œì‹œ ì¤€ìˆ˜"""
    
    # ê³„ì¸µ ë ˆë²¨ ì •ì˜
    HIERARCHY_LEVELS = {
        "ëŒ€ë¶„ë¥˜": 0,    # ë¶€ë¬¸ (ì˜ˆ: í† ëª©ë¶€ë¬¸, ê±´ì¶•ë¶€ë¬¸)
        "ì¤‘ë¶„ë¥˜": 1,    # ì¥ (ì˜ˆ: ì œ1ì¥, ì œ2ì¥)
        "ì†Œë¶„ë¥˜": 2,    # ì ˆ (ì˜ˆ: 1-1, 2-1)
        "ë³¸ë¬¸ë¶„ë¥˜": 3,  # ì¡°/í•­ëª© (ì˜ˆ: 1-1-1, 2-1-1)
        "ì„¸ë¶€ë¶„ë¥˜": 4   # ì„¸ë¶€í•­ëª© (ì˜ˆ: 1-1-1-1, 2-1-1-1)
    }
    
    # ë“¤ì—¬ì“°ê¸° ê·œì¹™ (ê³µë°± 2ê°œ ë‹¨ìœ„)
    INDENT_RULES = {
        0: "",           # ëŒ€ë¶„ë¥˜: ë“¤ì—¬ì“°ê¸° ì—†ìŒ
        1: "  ",         # ì¤‘ë¶„ë¥˜: 2ì¹¸ ë“¤ì—¬ì“°ê¸°
        2: "    ",       # ì†Œë¶„ë¥˜: 4ì¹¸ ë“¤ì—¬ì“°ê¸°
        3: "      ",     # ë³¸ë¬¸ë¶„ë¥˜: 6ì¹¸ ë“¤ì—¬ì“°ê¸°
        4: "        "    # ì„¸ë¶€ë¶„ë¥˜: 8ì¹¸ ë“¤ì—¬ì“°ê¸°
    }
    
    @classmethod
    def get_hierarchy_level(cls, item_text: str) -> int:
        """í•­ëª© í…ìŠ¤íŠ¸ë¡œë¶€í„° ê³„ì¸µ ë ˆë²¨ íŒë‹¨"""
        # ë¶€ë¬¸ íŒ¨í„´ (ëŒ€ë¶„ë¥˜)
        if re.match(r'^[ê°€-í£]+ë¶€ë¬¸$', item_text):
            return 0
        
        # ì¥ íŒ¨í„´ (ì¤‘ë¶„ë¥˜)
        if re.match(r'^ì œ\d+ì¥\s+', item_text):
            return 1
        
        # ì ˆ íŒ¨í„´ (ì†Œë¶„ë¥˜) - n-n í˜•íƒœ
        if re.match(r'^\d+-\d+$', item_text.split()[0]):
            return 2
        
        # ì¡°/í•­ëª© íŒ¨í„´ (ë³¸ë¬¸ë¶„ë¥˜) - n-n-n í˜•íƒœ
        if re.match(r'^\d+-\d+-\d+$', item_text.split()[0]):
            return 3
        
        # ì„¸ë¶€í•­ëª© íŒ¨í„´ (ì„¸ë¶€ë¶„ë¥˜) - n-n-n-n í˜•íƒœ
        if re.match(r'^\d+-\d+-\d+-\d+$', item_text.split()[0]):
            return 4
        
        # ê¸°ë³¸ê°’: ë³¸ë¬¸ë¶„ë¥˜
        return 3
    
    @classmethod
    def get_indent(cls, level: int) -> str:
        """ê³„ì¸µ ë ˆë²¨ì— ë”°ë¥¸ ë“¤ì—¬ì“°ê¸° ë°˜í™˜"""
        return cls.INDENT_RULES.get(level, "      ")
```

### 2. ê³„ì¸µ êµ¬ì¡° ê²€ì¦ ê·œì¹™ (MANDATORY)
```python
def validate_hierarchy_structure(toc_tree: dict) -> Dict[str, Any]:
    """ê³„ì¸µ êµ¬ì¡° ê²€ì¦ - ë°˜ë“œì‹œ ì‹¤í–‰"""
    validation_result = {
        "is_valid": True,
        "hierarchy_errors": [],
        "indent_errors": [],
        "level_consistency": True
    }
    
    def validate_node(node, expected_level=0):
        """ë…¸ë“œ ê³„ì¸µ êµ¬ì¡° ê²€ì¦"""
        current_level = node.get('level', 0)
        
        # ê³„ì¸µ ë ˆë²¨ ì¼ê´€ì„± ê²€ì¦
        if current_level != expected_level:
            validation_result["hierarchy_errors"].append({
                "node": node.get('title', 'Unknown'),
                "expected_level": expected_level,
                "actual_level": current_level
            })
            validation_result["is_valid"] = False
        
        # ë“¤ì—¬ì“°ê¸° ê²€ì¦
        expected_indent = TOCHierarchyRules.get_indent(current_level)
        actual_indent = node.get('indent', '')
        if expected_indent != actual_indent:
            validation_result["indent_errors"].append({
                "node": node.get('title', 'Unknown'),
                "expected_indent": expected_indent,
                "actual_indent": actual_indent
            })
            validation_result["is_valid"] = False
        
        # í•˜ìœ„ ë…¸ë“œ ê²€ì¦
        for child in node.get('children', []):
            validate_node(child, current_level + 1)
    
    # ë£¨íŠ¸ ë…¸ë“œë¶€í„° ê²€ì¦ ì‹œì‘
    for root_node in toc_tree.get('children', []):
        validate_node(root_node, 0)
    
    return validation_result
```

### 3. ê³„ì¸µ êµ¬ì¡° ìƒì„± ê·œì¹™ (MANDATORY)
```python
def generate_hierarchical_toc(toc_items: List[Dict]) -> Dict[str, Any]:
    """ê³„ì¸µì  ëª©ì°¨ íŠ¸ë¦¬ ìƒì„± - ë°˜ë“œì‹œ ì‚¬ìš©"""
    
    def create_node(title: str, page: int, level: int) -> Dict:
        """ê³„ì¸µ ë…¸ë“œ ìƒì„±"""
        return {
            'title': title,
            'page': page,
            'level': level,
            'indent': TOCHierarchyRules.get_indent(level),
            'children': []
        }
    
    def add_to_tree(tree: Dict, item: Dict):
        """íŠ¸ë¦¬ì— í•­ëª© ì¶”ê°€"""
        level = TOCHierarchyRules.get_hierarchy_level(item['title'])
        node = create_node(item['title'], item['page'], level)
        
        # ì ì ˆí•œ ë¶€ëª¨ ë…¸ë“œ ì°¾ê¸°
        current_level = level
        parent = tree
        
        # ìƒìœ„ ë ˆë²¨ ë…¸ë“œ ì°¾ê¸°
        while current_level > 0 and parent.get('children'):
            # ë§ˆì§€ë§‰ í•˜ìœ„ ë…¸ë“œê°€ ê°™ì€ ë ˆë²¨ì´ë©´ ê°™ì€ ë¶€ëª¨ì— ì¶”ê°€
            last_child = parent['children'][-1]
            if last_child['level'] == level:
                parent['children'].append(node)
                return
            
            # ìƒìœ„ ë ˆë²¨ ë…¸ë“œ ì°¾ê¸°
            for child in reversed(parent['children']):
                if child['level'] < level:
                    add_to_tree(child, item)
                    return
            
            current_level -= 1
        
        # ë£¨íŠ¸ ë ˆë²¨ì´ë©´ ì§ì ‘ ì¶”ê°€
        if level == 0:
            tree['children'].append(node)
        else:
            # ì ì ˆí•œ ë¶€ëª¨ê°€ ì—†ìœ¼ë©´ ë£¨íŠ¸ì— ì¶”ê°€
            tree['children'].append(node)
    
    # íŠ¸ë¦¬ ì´ˆê¸°í™”
    tree = {'children': []}
    
    # ëª¨ë“  í•­ëª©ì„ ê³„ì¸µ êµ¬ì¡°ë¡œ ì¶”ê°€
    for item in toc_items:
        add_to_tree(tree, item)
    
    return tree
```

### 4. ë§ˆí¬ë‹¤ìš´ ìƒì„± ì‹œ ê³„ì¸µ êµ¬ì¡° ì ìš© (MANDATORY)
```python
def render_hierarchical_markdown(node: Dict, depth: int = 0) -> str:
    """ê³„ì¸µì  ë§ˆí¬ë‹¤ìš´ ìƒì„± - ë°˜ë“œì‹œ ì‚¬ìš©"""
    md = ""
    
    # ë“¤ì—¬ì“°ê¸° ì ìš©
    indent = TOCHierarchyRules.get_indent(node['level'])
    
    # ë…¸ë“œ íƒ€ì…ì— ë”°ë¥¸ ì¶œë ¥ í˜•ì‹
    if node['type'] == 'chapter':
        md += f"{indent}- {node['title']} (p.{node['page']})\n"
    elif node['type'] == 'other':
        md += f"{indent}- {node['title']} (p.{node['page']})\n"
    else:
        md += f"{indent}- {node['number']} {node['title']} (p.{node['page']})\n"
    
    # í•˜ìœ„ ë…¸ë“œ ì¬ê·€ ì²˜ë¦¬
    for child in node.get('children', []):
        md += render_hierarchical_markdown(child, depth + 1)
    
    return md
```

### 5. ê³„ì¸µ êµ¬ì¡° ì˜¤ë¥˜ ë°©ì§€ ê·œì¹™ (MANDATORY)
```python
# ğŸš¨ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•  ê³„ì¸µ êµ¬ì¡° ì˜¤ë¥˜ ë°©ì§€ ê·œì¹™

# 1. ëª¨ë“  ëª©ì°¨ í•­ëª©ì€ ë°˜ë“œì‹œ ì ì ˆí•œ ê³„ì¸µ ë ˆë²¨ì„ ê°€ì ¸ì•¼ í•¨
# 2. ë“¤ì—¬ì“°ê¸°ëŠ” ê³„ì¸µ ë ˆë²¨ì— ë”°ë¼ ì •í™•íˆ ì ìš©ë˜ì–´ì•¼ í•¨
# 3. ìƒìœ„ í•­ëª© ì—†ì´ í•˜ìœ„ í•­ëª©ì´ ë…ë¦½ì ìœ¼ë¡œ ì¡´ì¬í•˜ë©´ ì•ˆ ë¨
# 4. ê³„ì¸µ êµ¬ì¡° ê²€ì¦ì€ ë°˜ë“œì‹œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨
# 5. ê²€ì¦ ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ê³„ì¸µ êµ¬ì¡° ì¬êµ¬ì„±í•´ì•¼ í•¨

def prevent_hierarchy_errors(toc_tree: dict) -> dict:
    """ê³„ì¸µ êµ¬ì¡° ì˜¤ë¥˜ ë°©ì§€ í•¨ìˆ˜ - ë°˜ë“œì‹œ ì‚¬ìš©"""
    # ê³„ì¸µ êµ¬ì¡° ê²€ì¦
    validation = validate_hierarchy_structure(toc_tree)
    
    # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¬êµ¬ì„±
    if not validation["is_valid"]:
        print("âš ï¸ ê²½ê³ : ê³„ì¸µ êµ¬ì¡° ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ê³„ì¸µ ì˜¤ë¥˜: {len(validation['hierarchy_errors'])}ê°œ")
        print(f"ë“¤ì—¬ì“°ê¸° ì˜¤ë¥˜: {len(validation['indent_errors'])}ê°œ")
        
        # ê³„ì¸µ êµ¬ì¡° ì¬êµ¬ì„±
        toc_tree = regenerate_hierarchical_structure(toc_tree)
        
        # ì¬ê²€ì¦
        validation = validate_hierarchy_structure(toc_tree)
    
    return toc_tree
```

**ğŸš¨ CRITICAL: ê³„ì¸µ êµ¬ì¡° ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ìœ„ì˜ ëª¨ë“  ê³„ì¸µ êµ¬ì¡° ê·œì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.**

## ğŸ—ï¸ Architecture Patterns

### 1. Dependency Injection Pattern
```python
class PDFProcessor:
    def __init__(self, use_lookup_table: bool = True):
        self.classifier = ContentClassifier()
        self.lookup_table = LookupTable() if use_lookup_table else None
```

### 2. Configuration Management
```python
# Always use config/settings.py for all configuration
from config.settings import (
    INPUT_DIR, 
    OUTPUT_DIR, 
    ensure_directories,
    check_environment
)
```

### 3. Modular Design
- **classifier/**: PDF classification logic
- **converter/**: PDF conversion tools  
- **utils/**: Utility functions
- **ect/**: Experimental/temporary tools

## ğŸ”§ Cross-Platform Development Guidelines

### 1. Environment Detection
```python
import platform
import sys

def is_windows() -> bool:
    """Windows í”Œë«í¼ í™•ì¸"""
    return platform.system().lower() == "windows"

def is_macos() -> bool:
    """macOS í”Œë«í¼ í™•ì¸"""
    return platform.system().lower() == "darwin"

def is_linux() -> bool:
    """Linux í”Œë«í¼ í™•ì¸"""
    return platform.system().lower() == "linux"
```

### 2. File System Operations
```python
def create_directories_safely(directory_paths: List[str]) -> bool:
    """ì•ˆì „í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
    try:
        for path in directory_paths:
            Path(path).mkdir(parents=True, exist_ok=True)
        return True
    except Exception as e:
        print(f"ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
        return False

def copy_files_cross_platform(source: Path, destination: Path) -> bool:
    """í¬ë¡œìŠ¤ í”Œë«í¼ íŒŒì¼ ë³µì‚¬"""
    try:
        import shutil
        shutil.copy2(source, destination)
        return True
    except Exception as e:
        print(f"íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {e}")
        return False
```

### 3. Process and Subprocess Handling
```python
import subprocess
import sys

def run_command_cross_platform(command: List[str]) -> subprocess.CompletedProcess:
    """í¬ë¡œìŠ¤ í”Œë«í¼ ëª…ë ¹ì–´ ì‹¤í–‰"""
    try:
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            encoding='utf-8',
            check=True
        )
        return result
    except subprocess.CalledProcessError as e:
        print(f"ëª…ë ¹ì–´ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return e
    except FileNotFoundError:
        print(f"ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {command[0]}")
        return None
```

### 4. Text Encoding and Line Endings
```python
def read_text_file_cross_platform(file_path: Path) -> str:
    """í¬ë¡œìŠ¤ í”Œë«í¼ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°"""
    encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr']
    
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding, newline='') as f:
                return f.read()
        except UnicodeDecodeError:
            continue
    
    raise UnicodeDecodeError(f"ì§€ì›ë˜ëŠ” ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

def write_text_file_cross_platform(file_path: Path, content: str) -> bool:
    """í¬ë¡œìŠ¤ í”Œë«í¼ í…ìŠ¤íŠ¸ íŒŒì¼ ì“°ê¸°"""
    try:
        with open(file_path, 'w', encoding='utf-8', newline='') as f:
            f.write(content)
        return True
    except Exception as e:
        print(f"íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨: {e}")
        return False
```

### 5. Temporary File Handling
```python
import tempfile
from contextlib import contextmanager

@contextmanager
def create_temp_file_cross_platform(suffix: str = ".tmp") -> Path:
    """í¬ë¡œìŠ¤ í”Œë«í¼ ì„ì‹œ íŒŒì¼ ìƒì„±"""
    temp_file = None
    try:
        temp_file = Path(tempfile.mktemp(suffix=suffix))
        yield temp_file
    finally:
        if temp_file and temp_file.exists():
            temp_file.unlink()

def cleanup_temp_files(temp_dir: Path) -> None:
    """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
    try:
        for temp_file in temp_dir.glob("*.tmp"):
            temp_file.unlink()
    except Exception as e:
        print(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
```

## ğŸ“Š Document Analysis Best Practices

### 1. Multi-Step Analysis Process
```python
def comprehensive_document_analysis(file_path: Path) -> Dict[str, Any]:
    """ì¢…í•©ì ì¸ ë¬¸ì„œ ë¶„ì„"""
    results = {
        "file_info": {},
        "structure": {},
        "content_analysis": {},
        "validation": {},
        "recommendations": []
    }
    
    # 1ë‹¨ê³„: íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
    results["file_info"] = collect_file_info(file_path)
    
    # 2ë‹¨ê³„: êµ¬ì¡° ë¶„ì„
    results["structure"] = analyze_document_structure(file_path)
    
    # 3ë‹¨ê³„: ë‚´ìš© ë¶„ì„
    results["content_analysis"] = analyze_content(file_path)
    
    # 4ë‹¨ê³„: ê²€ì¦
    results["validation"] = validate_analysis_results(results)
    
    # 5ë‹¨ê³„: ê¶Œì¥ì‚¬í•­ ìƒì„±
    results["recommendations"] = generate_recommendations(results)
    
    return results
```

### 2. Pattern Recognition
```python
def identify_document_patterns(content: str) -> Dict[str, List[str]]:
    """ë¬¸ì„œ íŒ¨í„´ ì‹ë³„"""
    patterns = {
        "sections": [],
        "chapters": [],
        "subsections": [],
        "page_breaks": [],
        "headers": [],
        "footers": []
    }
    
    # ì •ê·œì‹ íŒ¨í„´ ì •ì˜
    import re
    
    # ë¶€ë¬¸ íŒ¨í„´
    section_pattern = r"([ê°€-í£]+ë¶€ë¬¸)"
    patterns["sections"] = re.findall(section_pattern, content)
    
    # ì¥ íŒ¨í„´
    chapter_pattern = r"(ì œ[0-9]+ì¥\s*[ê°€-í£]+)"
    patterns["chapters"] = re.findall(chapter_pattern, content)
    
    # ì ˆ íŒ¨í„´
    subsection_pattern = r"([0-9]+-[0-9]+[ê°€-í£]*)"
    patterns["subsections"] = re.findall(subsection_pattern, content)
    
    # í˜ì´ì§€ êµ¬ë¶„ íŒ¨í„´
    page_pattern = r"(=== [0-9]+í˜ì´ì§€ ===)"
    patterns["page_breaks"] = re.findall(page_pattern, content)
    
    return patterns
```

### 3. Content Validation
```python
def validate_content_analysis(analysis_results: Dict[str, Any]) -> Dict[str, bool]:
    """ë‚´ìš© ë¶„ì„ ê²€ì¦"""
    validation = {
        "structure_complete": False,
        "content_consistent": False,
        "encoding_valid": False,
        "patterns_reasonable": False
    }
    
    # êµ¬ì¡° ì™„ì„±ë„ ê²€ì¦
    if analysis_results.get("structure", {}).get("sections"):
        validation["structure_complete"] = True
    
    # ë‚´ìš© ì¼ê´€ì„± ê²€ì¦
    content = analysis_results.get("content", "")
    if content and len(content) > 100:
        validation["content_consistent"] = True
    
    # ì¸ì½”ë”© ê²€ì¦
    try:
        content.encode('utf-8')
        validation["encoding_valid"] = True
    except UnicodeEncodeError:
        pass
    
    # íŒ¨í„´ í•©ë¦¬ì„± ê²€ì¦
    patterns = analysis_results.get("patterns", {})
    if patterns and any(len(v) > 0 for v in patterns.values()):
        validation["patterns_reasonable"] = True
    
    return validation
```

## ğŸ¯ Quality Assurance

### 1. Code Review Checklist
- [ ] ëª¨ë“  í•¨ìˆ˜ì— íƒ€ì… íŒíŠ¸ í¬í•¨
- [ ] í•œêµ­ì–´ ì˜¤ë¥˜ ë©”ì‹œì§€ ì‚¬ìš©
- [ ] í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ì„± í™•ì¸
- [ ] ë¬¸ì„œ ë¶„ì„ íŒ¨í„´ ì ìš©
- [ ] ê²€ì¦ ë¡œì§ í¬í•¨
- [ ] ì˜ˆì™¸ ì²˜ë¦¬ ì™„ë£Œ

### 2. Testing Requirements
- [ ] Windows í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸
- [ ] macOS/Linux í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸
- [ ] í•œêµ­ì–´ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸
- [ ] ì˜¤ë¥˜ ìƒí™© í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### 3. Documentation Standards
- [ ] ëª¨ë“  ëª¨ë“ˆì— í•œêµ­ì–´ ì„¤ëª…
- [ ] ì‚¬ìš© ì˜ˆì œ í¬í•¨
- [ ] ì˜¤ë¥˜ í•´ê²° ë°©ë²• ë¬¸ì„œí™”
- [ ] í¬ë¡œìŠ¤ í”Œë«í¼ ê°€ì´ë“œ í¬í•¨

## ğŸš¨ CRITICAL DOCUMENT ANALYSIS VALIDATION RULES (MANDATORY - NEW)

### 1. Section Completeness Validation (MANDATORY)
```python
def validate_section_completeness(content: str) -> Dict[str, Any]:
    """ë¶€ë¬¸ ì™„ì „ì„± ê²€ì¦ - ë°˜ë“œì‹œ ì‹¤í–‰"""
    required_sections = ["ê³µí†µë¶€ë¬¸", "í† ëª©ë¶€ë¬¸", "ê±´ì¶•ë¶€ë¬¸", "ê¸°ê³„ì„¤ë¹„ë¶€ë¬¸", "ìœ ì§€ê´€ë¦¬ë¶€ë¬¸"]
    validation_result = {
        "is_complete": True,
        "missing_sections": [],
        "found_sections": [],
        "section_positions": {},
        "validation_errors": []
    }
    
    # ê° ë¶€ë¬¸ ê²€ìƒ‰ ë° ìœ„ì¹˜ ê¸°ë¡
    for section in required_sections:
        section_pos = content.find(section)
        if section_pos != -1:
            validation_result["found_sections"].append(section)
            validation_result["section_positions"][section] = section_pos
        else:
            validation_result["missing_sections"].append(section)
            validation_result["is_complete"] = False
    
    # ë¶€ë¬¸ ìˆœì„œ ê²€ì¦
    if len(validation_result["found_sections"]) > 1:
        positions = validation_result["section_positions"]
        sorted_sections = sorted(positions.items(), key=lambda x: x[1])
        validation_result["section_order"] = [section for section, _ in sorted_sections]
    
    return validation_result
```

### 2. Cross-Reference Validation (MANDATORY)
```python
def validate_cross_references(content: str, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
    """í¬ë¡œìŠ¤ ì°¸ì¡° ê²€ì¦ - ë°˜ë“œì‹œ ì‹¤í–‰"""
    validation_result = {
        "cross_references_valid": True,
        "inconsistencies": [],
        "recommendations": []
    }
    
    # ë¶€ë¬¸ ì œëª©ê³¼ ì¥ ì œëª©ì˜ ì¼ê´€ì„± ê²€ì¦
    sections = analysis_results.get("sections", {})
    patterns = analysis_results.get("patterns", {})
    
    for section_name, section_data in sections.items():
        if section_data.get("start") is not None:
            # í•´ë‹¹ ë¶€ë¬¸ì˜ ì¥ë“¤ì´ ì‹¤ì œë¡œ ê·¸ ë¶€ë¬¸ì— ì†í•˜ëŠ”ì§€ í™•ì¸
            section_start = section_data["start"]
            section_end = section_data.get("end")
            
            if section_end:
                section_content = content[section_start:section_end]
                chapters_in_section = patterns.get("chapters", [])
                
                # ë¶€ë¬¸ë³„ ì¥ ë¶„ë¥˜ ê²€ì¦
                for chapter in chapters_in_section:
                    if chapter not in section_content:
                        validation_result["inconsistencies"].append(
                            f"ì¥ '{chapter}'ì´ ë¶€ë¬¸ '{section_name}'ì— ì†í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
                        )
                        validation_result["cross_references_valid"] = False
    
    return validation_result
```

### 3. Analysis Report Validation (MANDATORY)
```python
def validate_analysis_report(report_content: str, original_analysis: Dict[str, Any]) -> Dict[str, Any]:
    """ë¶„ì„ ë³´ê³ ì„œ ê²€ì¦ - ë°˜ë“œì‹œ ì‹¤í–‰"""
    validation_result = {
        "report_complete": True,
        "missing_elements": [],
        "inconsistencies": [],
        "recommendations": []
    }
    
    # í•„ìˆ˜ ì„¹ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    required_report_sections = [
        "ë¶„ì„ ê°œìš”", "ë¬¸ì„œ êµ¬ì¡° ë¶„ì„", "ë¶€ë¬¸ë³„ ìƒì„¸ ë¶„ì„", 
        "ê²€ì¦ ê²°ê³¼", "ê¶Œì¥ì‚¬í•­"
    ]
    
    for section in required_report_sections:
        if section not in report_content:
            validation_result["missing_elements"].append(section)
            validation_result["report_complete"] = False
    
    # ë¶„ì„ ê²°ê³¼ì™€ ë³´ê³ ì„œ ë‚´ìš© ì¼ì¹˜ì„± í™•ì¸
    sections = original_analysis.get("sections", {})
    for section_name in sections.keys():
        if section_name not in report_content:
            validation_result["inconsistencies"].append(
                f"ë¶€ë¬¸ '{section_name}'ì´ ë³´ê³ ì„œì— ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤"
            )
            validation_result["report_complete"] = False
    
    return validation_result
```

### 4. Mandatory Analysis Workflow (MANDATORY)
```python
def execute_mandatory_analysis_workflow(file_path: Path) -> Dict[str, Any]:
    """í•„ìˆ˜ ë¶„ì„ ì›Œí¬í”Œë¡œìš° - ë°˜ë“œì‹œ ì‚¬ìš©"""
    try:
        # 1ë‹¨ê³„: íŒŒì¼ ë‚´ìš© ë¡œë“œ
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # 2ë‹¨ê³„: ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰
        analyzer = DocumentAnalyzer(file_path)
        analysis_results = analyzer.analyze_document_structure()
        
        # 3ë‹¨ê³„: ë¶€ë¬¸ ì™„ì „ì„± ê²€ì¦
        section_validation = validate_section_completeness(content)
        
        # 4ë‹¨ê³„: í¬ë¡œìŠ¤ ì°¸ì¡° ê²€ì¦
        cross_ref_validation = validate_cross_references(content, analysis_results)
        
        # 5ë‹¨ê³„: ë³´ê³ ì„œ ìƒì„±
        report_generator = AnalysisReportGenerator(analysis_results, section_validation)
        comprehensive_report = report_generator.generate_comprehensive_report()
        
        # 6ë‹¨ê³„: ë³´ê³ ì„œ ê²€ì¦
        report_validation = validate_analysis_report(comprehensive_report, analysis_results)
        
        # 7ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ë°˜í™˜
        final_result = {
            "analysis_results": analysis_results,
            "section_validation": section_validation,
            "cross_ref_validation": cross_ref_validation,
            "report_validation": report_validation,
            "comprehensive_report": comprehensive_report,
            "is_valid": all([
                section_validation["is_complete"],
                cross_ref_validation["cross_references_valid"],
                report_validation["report_complete"]
            ])
        }
        
        # 8ë‹¨ê³„: ê²€ì¦ ì‹¤íŒ¨ ì‹œ ê²½ê³  ì¶œë ¥
        if not final_result["is_valid"]:
            print("âš ï¸ ê²½ê³ : ë¶„ì„ ê²€ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
            if section_validation["missing_sections"]:
                print(f"ëˆ„ë½ëœ ë¶€ë¬¸: {', '.join(section_validation['missing_sections'])}")
            if cross_ref_validation["inconsistencies"]:
                print(f"ì¼ê´€ì„± ì˜¤ë¥˜: {len(cross_ref_validation['inconsistencies'])}ê°œ")
            if report_validation["missing_elements"]:
                print(f"ë³´ê³ ì„œ ëˆ„ë½ ìš”ì†Œ: {', '.join(report_validation['missing_elements'])}")
        
        return final_result
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {
            "error": str(e),
            "is_valid": False
        }
```

### 5. Error Prevention Rules (MANDATORY)
```python
# ğŸš¨ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•  ì˜¤ë¥˜ ë°©ì§€ ê·œì¹™

# 1. ëª¨ë“  ë¬¸ì„œ ë¶„ì„ì€ execute_mandatory_analysis_workflow() ì‚¬ìš©
# 2. ë¶„ì„ ê²°ê³¼ëŠ” ë°˜ë“œì‹œ validate_section_completeness() ê²€ì¦
# 3. ë³´ê³ ì„œ ìƒì„± í›„ validate_analysis_report() ê²€ì¦
# 4. ëˆ„ë½ëœ ë¶€ë¬¸ì´ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¬ë¶„ì„ ìˆ˜í–‰
# 5. ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ìì—ê²Œ ëª…í™•í•œ ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥

def prevent_analysis_errors(file_path: Path) -> Dict[str, Any]:
    """ë¶„ì„ ì˜¤ë¥˜ ë°©ì§€ í•¨ìˆ˜ - ë°˜ë“œì‹œ ì‚¬ìš©"""
    # í•„ìˆ˜ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    result = execute_mandatory_analysis_workflow(file_path)
    
    # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ìë™ ì¬ë¶„ì„
    if not result.get("is_valid", False):
        print("ğŸ”„ ê²€ì¦ ì‹¤íŒ¨ë¡œ ì¸í•œ ì¬ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        # ì¬ë¶„ì„ ë¡œì§
        result = execute_mandatory_analysis_workflow(file_path)
    
    return result
```

**ğŸš¨ CRITICAL: ê±´ì¶•ë¶€ë¬¸ ëˆ„ë½ ì‚¬ê³ ì™€ ê°™ì€ ë¶„ì„ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ìœ„ì˜ ëª¨ë“  ê²€ì¦ ê·œì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.**

### 9-1. ê°€ì„¤ê³µì‚¬ ë¶„ë¥˜ ê·œì¹™ (MANDATORY)
```python
# "ì œ2ì¥ ê°€ì„¤ê³µì‚¬"ëŠ” ë°˜ë“œì‹œ "ê³µí†µë¶€ë¬¸"ì— í¬í•¨ì‹œì¼œì•¼ í•œë‹¤.
# ëª©ì°¨ íŠ¸ë¦¬ ìƒì„± ë° PDF ë¶„í•  ì‹œ ì•„ë˜ ê·œì¹™ì„ ìš°ì„  ì ìš©í•  ê²ƒ.

SPECIAL_SECTION_MAPPING = {
    "ì œ2ì¥ ê°€ì„¤ê³µì‚¬": "ê³µí†µë¶€ë¬¸"
}

def get_section_bumun(chapter_title: str) -> str:
    """íŠ¹ì • ì¥ì˜ ë¶€ë¬¸ì„ ê°•ì œ ì§€ì •"""
    if chapter_title in SPECIAL_SECTION_MAPPING:
        return SPECIAL_SECTION_MAPPING[chapter_title]
    # ê¸°ë³¸ ë¶„ë¥˜ ë¡œì§...
```
**ALWAYS** check for special section mapping before default classification.
**NEVER** classify "ê°€ì„¤ê³µì‚¬" as í† ëª©ë¶€ë¬¸.













